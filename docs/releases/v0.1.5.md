# Gonimbus v0.1.5 Release Notes

**Release Date**: 2026-01-23
**Status**: Content Streaming + validate=size for Consumer Integration

## Overview

Gonimbus v0.1.5 introduces content streaming commands and validation, enabling Gonimbus to serve as a data plane for downstream consumers that need to process object content without managing provider SDKs directly.

Key capabilities:

1. **Content Streaming Commands** (`stream get`, `stream head`) - structured access to object metadata and content with JSONL framing
2. **validate=size** - early detection of stale index/list metadata before deep pipeline processing
3. **Language-neutral contract** (ADR-0004) - helpers can be implemented in any language (Go, Python, Node, C#)

## The Problem

Applications processing cloud object data often need to:

- **Retrieve metadata** for routing decisions without downloading content
- **Stream content** through processing pipelines without buffering entire objects
- **Detect staleness** when cached indexes don't match current bucket state
- **Handle errors** in structured form for automated retry/alerting

Traditional approaches require each consumer to implement provider SDK integration, credential handling, and error classification.

## The Solution: `stream` Commands

### `gonimbus stream head`

Retrieves object metadata without downloading content:

```bash
gonimbus stream head s3://bucket/key --profile my-profile
```

Output is a single `gonimbus.object.v1` JSONL record:

```json
{
  "type": "gonimbus.object.v1",
  "ts": "2026-01-23T12:28:17.349Z",
  "job_id": "...",
  "provider": "s3",
  "data": {
    "key": "path/to/object.xml",
    "size": 3729736,
    "etag": "60eda68512f8238bd2ba9abac0de63d7",
    "last_modified": "2025-12-15T20:53:44Z",
    "content_type": "application/xml",
    "metadata": { "custom": "user-metadata" }
  }
}
```

### `gonimbus stream get`

Streams object content with JSONL framing (mixed-framing output):

```bash
gonimbus stream get s3://bucket/key --profile my-profile
```

Output sequence:

1. `gonimbus.stream.open.v1` - metadata (uri, size, etag, last_modified)
2. `gonimbus.stream.chunk.v1` + raw bytes - repeated for each chunk
3. `gonimbus.stream.close.v1` - completion status

Example output structure:

```
{"type":"gonimbus.stream.open.v1",...,"data":{"stream_id":"...","uri":"s3://...","size":3729736,...}}
{"type":"gonimbus.stream.chunk.v1",...,"data":{"stream_id":"...","seq":0,"nbytes":65536}}
<65536 raw bytes>
{"type":"gonimbus.stream.chunk.v1",...,"data":{"stream_id":"...","seq":1,"nbytes":65536}}
<65536 raw bytes>
...
{"type":"gonimbus.stream.close.v1",...,"data":{"stream_id":"...","status":"success","chunks":58,"bytes":3729736}}
```

### Streaming Error Contract

Errors are emitted to **stdout** as `gonimbus.error.v1` records:

```json
{
  "type": "gonimbus.error.v1",
  "data": {
    "code": "NOT_FOUND",
    "message": "...",
    "key": "...",
    "details": { "mode": "streaming" }
  }
}
```

This enables consumers to rely on structured output without scraping stderr.

## Stream Contract (ADR-0004)

The streaming output follows a language-neutral contract documented in ADR-0004:

| Record Type       | Required Fields                  | Notes                                            |
| ----------------- | -------------------------------- | ------------------------------------------------ |
| `stream.open.v1`  | stream_id, uri                   | size, etag, last_modified, content_type optional |
| `stream.chunk.v1` | stream_id, seq, nbytes           | Raw bytes follow immediately                     |
| `stream.close.v1` | stream_id, status, chunks, bytes | status: success/error/cancelled                  |

### Why Mixed Framing?

- **No base64 overhead**: Raw bytes are emitted directly after chunk headers
- **Incremental decode**: Consumers read one JSON line, then N bytes, repeat
- **Any language**: Contract is implementable in Go, Python, Node, Rust, C#

### Decoder Package

The `pkg/stream` package provides Go helpers:

```go
import "github.com/3leaps/gonimbus/pkg/stream"

d := stream.NewDecoder(r)
for {
    ev, err := d.Next()
    if err == io.EOF {
        break
    }
    if ev.Kind == stream.EventChunk {
        // ev.Chunk.Body is an io.ReadCloser for the raw bytes
        io.Copy(dst, ev.Chunk.Body)
        ev.Chunk.Body.Close()
    }
}
```

Truncation is detected: if the stream is cut mid-chunk, `Decoder` returns `io.ErrUnexpectedEOF`.

## validate=size (Stale Index Mitigation)

Both `stream get` and transfer operations validate that enumerated size matches actual content-length:

```
Enumerated size (from list/index): 3729736
GetObject content-length:          3729736  âœ“
```

When sizes don't match:

- **Error emitted immediately** with `NOT_FOUND` code (stale key semantics)
- **No deep pipeline processing** - avoids wasted buffering and retries
- **Clear error message**: `source size mismatch for <key>: expected=N got=M`

### When This Helps

- Index was built yesterday; object was replaced today
- List operation returned stale metadata from eventually-consistent view
- Object was deleted and recreated with different content

## Performance

Stream commands add minimal overhead to raw provider operations:

| Operation   | Overhead         | Notes                     |
| ----------- | ---------------- | ------------------------- |
| stream head | ~1 HEAD request  | Metadata only             |
| stream get  | ~1 HEAD + 1 GET  | Size validation adds HEAD |
| Chunking    | ~200 bytes/chunk | JSONL headers             |

Tested with enterprise data:

| File Size | Chunks | Throughput |
| --------- | ------ | ---------- |
| 466 B     | 1      | instant    |
| 3.3 MB    | 403    | ~1.5 MB/s  |
| 3.7 MB    | ~58    | ~2 MB/s    |

## Changes

### Added

**Stream Commands:**

- `gonimbus stream get <uri>` - stream object content with JSONL framing
- `gonimbus stream head <uri>` - retrieve object metadata
- Errors on stdout as `gonimbus.error.v1` (streaming mode contract)
- Size validation (HEAD vs GetObject) with NOT_FOUND mapping

**Stream Package (`pkg/stream/`):**

- `Writer` for producing mixed-framing streams
- `Decoder` for consuming streams with truncation detection
- `Open`, `Chunk`, `Close` types for stream records

**Transfer:**

- `SizeMismatchError` type for stale index/list detection
- Error classification maps size mismatch to `NOT_FOUND`

**Documentation:**

- ADR-0004: Language-neutral content stream contract
- Streaming contract specification (`docs/development/streaming/`)
- QA checklist and helper replication guidance

### Fixed

- Cloud integration test credentials for stream writer tests

## Upgrade Notes

No breaking changes from v0.1.4. Upgrade with:

```bash
go install github.com/3leaps/gonimbus/cmd/gonimbus@v0.1.5
```

## Use Cases

### Metadata-Based Routing

Use `stream head` to make routing decisions without downloading content:

```bash
# Check file type before processing
gonimbus stream head s3://bucket/data/file.xml --profile prod | jq '.data.content_type'
```

### Content Pipeline Integration

Pipe `stream get` to downstream processors:

```bash
# Extract and process with custom decoder
gonimbus stream get s3://bucket/data/file.xml --profile prod | ./my-decoder | ./processor
```

### Integrity Verification

Compare streamed content against expected hash:

```bash
# Stream and compute MD5
gonimbus stream get s3://bucket/file --profile prod 2>/dev/null | ./extract-stream | md5
```

## What's Next

v0.1.6 will focus on:

- Python helper package for stream decode
- Range request support (`--range start-end`)
- GCS provider support (fast-follow)

## Contributors

- Dave Thompson (@3leapsdave)

## License

Apache License 2.0 - see [LICENSE](../../LICENSE)
